{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac4621c-2cd2-4898-be14-c32661f21d08",
   "metadata": {},
   "source": [
    "##### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd7de925-ce02-45ea-81cd-16edeee1a57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb93ad-9bc1-443a-82c8-528cb4848e12",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f77f883-eb92-4337-a2af-06dfdaa46d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2Config\n",
    "from transformers import GPT2Model\n",
    "import transformers \n",
    "import datasets\n",
    "import logging\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a28438c-8403-4c45-be2d-c904f60e6f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cdf6ea3-9cee-4bcf-b76e-ba08d4537519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using transformers version: 4.18.0]\n",
      "[Using transformers version: 4.18.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 20:18:19,799 - sagemaker - INFO - [Using transformers version: 4.18.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using datasets version: 2.4.0]\n",
      "[Using datasets version: 2.4.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 20:18:19,801 - sagemaker - INFO - [Using datasets version: 2.4.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using torch version: 1.8.1]\n",
      "[Using torch version: 1.8.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 20:18:19,802 - sagemaker - INFO - [Using torch version: 1.8.1]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using transformers version: {transformers.__version__}]')\n",
    "logger.info(f'[Using datasets version: {datasets.__version__}]')\n",
    "logger.info(f'[Using torch version: {torch.__version__}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628eada1-35c1-4f18-a0d3-b4a2ad45be89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94101ad-ab61-40eb-bb9d-6d5a081057de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9a5f663-c74a-487d-a320-8923a36a8399",
   "metadata": {},
   "source": [
    "#### Explore the config \n",
    "https://huggingface.co/docs/transformers/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e95c46-33d0-4e17-ac74-d7441622e7c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = GPT2Config()\n",
    "# Initializing a model (with random weights) from the configuration\n",
    "model = GPT2Model(configuration)\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a81ec-4bea-4c3b-acb2-9a7f7435e37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34b79d-1ea8-4eb5-9558-9193a6cfb880",
   "metadata": {},
   "source": [
    "#### Explore the tokenizer \n",
    "Type of tokenization = byte-level byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cad8c6-e31e-4f0d-8a9e-7779258c6324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2438c-9095-471a-9b03-aa296a6eb5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer('Happy friday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc311524-357a-4ec9-b2fc-ecb14bcaadab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer('I truly believe in eternal love!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7397a-a625-4381-aa64-75cd93b09f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_vocabulary('./data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302a345-8de1-4c81-af6f-798f24879706",
   "metadata": {},
   "source": [
    "#### Explore tokenizer fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98903705-8542-484e-aa0e-da584dccd01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1418-7282-467a-958b-d28695a45369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer('Happy friday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b1479-1f4a-4571-9170-6a163e534a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer('I truly believe in eternal love!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626cec9-0d1c-442a-a13a-583b07a5efa0",
   "metadata": {},
   "source": [
    "#### Explore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec26f7c-d2e6-4afd-b67e-d7470e673c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # using the default tokenizer\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64270fc1-a07a-4f3f-a358-d933fe6f178c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer('Happy friday', return_tensors='pt')\n",
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef714df-2e46-44c1-9c34-684bdf49554e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f8cbb-85b8-4bfb-b411-2d7355f96fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2794cce-ab51-468e-80f6-1c5f60e86540",
   "metadata": {},
   "source": [
    "#### Get number of attention heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83679e31-7a4c-4bc2-a2c6-a53a94d41085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of heads in the model\n",
    "num_heads = model.config.num_attention_heads\n",
    "num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6015c8d-1074-420a-b127-fee48efc93cf",
   "metadata": {},
   "source": [
    "#### Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf13cf-38df-4e3a-820b-e1d9021e5835",
   "metadata": {},
   "source": [
    "The tokenizer from the \"microsoft/DialogRPT-updown\" checkpoint is fine-tuned by Microsoft on conversational data and it is trained to work with the DialogRPT-updown model which is a conversational language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c09c1b-864e-46ae-9927-14c4f45a3f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialogRPT-updown')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6880bc-183e-44a1-9427-a33fdcf11f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained('microsoft/DialogRPT-updown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba5e6a-a68c-4fe5-9e8d-46209be36d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer('Hello, my dog is cute', return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f491c-f4ab-410e-915a-defd132c742b",
   "metadata": {},
   "source": [
    "In this example, the model is evaluated with the inputs, but the gradients are not computed because we are inside the context of torch.no_grad().\n",
    "This will save memory and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b2d01-5c8a-4014-89bd-e44cf77c99dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6209db-59c5-4d39-9eab-b6112162cacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0cf23-004d-4dfa-a0b8-afad63e8af44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481c65f-20c8-4a44-9f84-269972007a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = len(model.config.id2label)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b462bc4-7144-4e46-b67f-aa8af6903a20",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "* Itâ€™s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d226ad-da25-4f94-adcf-3d859c750964",
   "metadata": {},
   "source": [
    "### Code start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f59cd0-4bba-4c9c-a116-96aa7bb5146c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from pathlib import Path\n",
    "import transformers \n",
    "import tokenizers\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.getLevelName('INFO'), \n",
    "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log versions of dependencies\n",
    "logger.info(f'[Using Transformers: {transformers.__version__}]')\n",
    "logger.info(f'[Using Tokenizers: {tokenizers.__version__}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc6404-d7ba-430f-ac05-f0c64cbb76c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "default_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764849b5-a420-4c35-b39b-f888138b7af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = default_tokenizer.vocab_size\n",
    "model_max_length = default_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ce3a2-dce9-4250-9923-6b03155c9295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCAL_INPUT_PATH = './corpus/' \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the custom vocabulary after training the tokenizer\n",
    "LOCAL_OUTPUT_PATH = './data1/'\n",
    "\n",
    "\n",
    "# Read input files from local input path \n",
    "logger.info(f'Reading input files from [{LOCAL_INPUT_PATH}/]')\n",
    "paths = [str(x) for x in Path(LOCAL_INPUT_PATH).glob('*.txt')]\n",
    "print(paths)\n",
    "\n",
    "# Train custom BertWordPiece tokenizer\n",
    "logger.info(f'Training BytePair custom tokenizer using files in {paths}')\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "print(tokenizer)\n",
    "\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=vocab_size, \n",
    "                min_frequency=1, \n",
    "                special_tokens=['<|endoftext|>'])\n",
    "tokenizer.enable_truncation(max_length=1024)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d463f9-0678-4f5b-a7a8-f7cefea516c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.save_model(LOCAL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23009ad4-ae37-4f6b-b7f8-09c008eaaf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075f905-a492-41a8-a08d-1ef93958bb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-create custom tokenizer using vocab from local output path\n",
    "logger.info(f'Re-create GPT2Tokenizer custom tokenizer using extracted custom vocab in {LOCAL_OUTPUT_PATH}')\n",
    "custom_tokenizer = GPT2TokenizerFast.from_pretrained('./data1', pad_token='<|endoftext|>')\n",
    "custom_tokenizer.model_max_length = 1024\n",
    "print(custom_tokenizer)\n",
    "\n",
    "\n",
    "# Evaluate custom tokenizer \n",
    "logger.info('Evaluating custom tokenizer')\n",
    "test_sentence = 'covid virus in usa'\n",
    "logger.info(f'Test sentence: {test_sentence}')\n",
    "tokens = tokenizer.encode(test_sentence).tokens\n",
    "logger.info(f'Encoded sentence: {tokens}')\n",
    "token_id = tokenizer.token_to_id('covid')\n",
    "logger.info(f'Token ID for token (covid) = {token_id}')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "logger.info(f'Vocabulary size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32642c8f-b11e-40cc-9cd6-f813d871bcbe",
   "metadata": {},
   "source": [
    "#### Tokenize 10k articles using the custom tokenizer we build now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912d9395-cf57-43ed-85bf-96e773648065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:28:34,795 - __main__ - INFO - [Using Transformers: 4.18.0]\n",
      "2023-01-21 19:28:34,796 - __main__ - INFO - [Using Datasets: 2.4.0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import GPT2Config\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from pathlib import Path\n",
    "import transformers \n",
    "import datasets\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.getLevelName('INFO'), \n",
    "                    handlers=[logging.StreamHandler(sys.stdout)], \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Log versions of dependencies\n",
    "logger.info(f'[Using Transformers: {transformers.__version__}]')\n",
    "logger.info(f'[Using Datasets: {datasets.__version__}]')\n",
    "\n",
    "# Essentials\n",
    "# LOCAL_INPUT_PATH is mapped to S3 input location for covid news articles \n",
    "LOCAL_INPUT_PATH = './corpus' \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the processed input data (COVID articles)\n",
    "LOCAL_OUTPUT_PATH = '/tokenized'\n",
    "MAX_LENGTH = 512\n",
    "CHUNK_SIZE = 128\n",
    "N_GPUS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b7e5b6c-32fb-4b08-851f-ed9480c68263",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "146ff51c-a062-416c-b541-78596ee3b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('./data1', pad_token='<|endoftext|>')\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5776b603-170f-493c-bb32-ddee4ae47c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='./data1', vocab_size=50257, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcf76c1f-aa53-4f3d-912f-b20a6657d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:28:37,392 - __main__ - INFO - Reading and collating input data to create mini batches for Causal Language Model (CLM) training\n",
      "2023-01-21 19:28:37,440 - datasets.builder - WARNING - Using custom data configuration default-a847095289281709\n",
      "2023-01-21 19:28:37,442 - datasets.builder - WARNING - Reusing dataset text (/tmp/cache/text/default-a847095289281709/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n",
      "2023-01-21 19:28:37,443 - __main__ - INFO - Dataset: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10001\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Read dataset and collate to create mini batches for Causal Language Model (CLM) training\n",
    "logger.info('Reading and collating input data to create mini batches for Causal Language Model (CLM) training')\n",
    "dataset = load_dataset('text', data_files='./corpus/covid_articles.txt', split='train', cache_dir='/tmp/cache')\n",
    "logger.info(f'Dataset: {dataset}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3929b975-b4c2-4ef5-87f6-f4646b78de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:28:39,239 - __main__ - INFO - Splitting dataset into train and validation splits\n",
      "2023-01-21 19:28:39,241 - datasets.arrow_dataset - WARNING - Loading cached split indices for dataset at /tmp/cache/text/default-a847095289281709/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-038f7ae13fb217c2.arrow and /tmp/cache/text/default-a847095289281709/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-e7076fe0c2be1d5f.arrow\n",
      "2023-01-21 19:28:39,244 - __main__ - INFO - Data splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1001\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and validation splits \n",
    "logger.info('Splitting dataset into train and validation splits')\n",
    "train_test_splits = dataset.train_test_split(shuffle=True, seed=123, test_size=0.1)\n",
    "data_splits = DatasetDict({'train': train_test_splits['train'], \n",
    "                           'validation': train_test_splits['test']})\n",
    "logger.info(f'Data splits: {data_splits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d653d289-eced-49d4-856e-8b0e6b6dc4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:28:40,532 - __main__ - INFO - Tokenizing dataset splits\n",
      "2023-01-21 19:28:40,534 - __main__ - INFO - Total number of processes = 32\n",
      "                                      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09127b1a28948dea173be778aad6bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258b51412c614c6cae0409c4e5d3c48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4ee2655da74194a1004fa5af2f5c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06530a365f004f338592557b66ffd668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa983561954f44478028efd42fea4717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01571547f2840a291154119a12055bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d61aea1725431283050df6d172bd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6aa009bfb7490a928d6d987dc03b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64d3583d03a45dabdb7df79f29193ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8638dabc6cf041b39bd0d6e1747a65c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3edb26e98e944ef9bd627c7a1cd5ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3954e0f10dcf44c98b827e65f8297a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91627fa27eae461bb561a9653ad11f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960655c33d5a4ec39480a59f065ddf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded4c7c1741e4246807de90be059ad4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4650950f95864faeb2f5eb12707af9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc85779f047406ca91c3f3ce2ff954f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#16:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d24bf3625a416cbdca537005c8cc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#17:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c22519548e480f92b7122fb1e0ac6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#18:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117903d9302d4514905e6d7b63bd43ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#20:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec7fd7047ac46e9adbb6cd6f4475b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#19:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c595513fac9e40ae91f28c7ae29ecde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#24:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aadf1ef66414e7cb251e481c6562546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#21:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc9c2f76d96466f92820d9b4a57e499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#25:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94de35d1d69e4212938e4b9725993948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#28:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c603146a29524303a9fda03545892b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#22:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96facf5c70ad4ecd8149e0e6466b5639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#27:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fde65ffd7a4ebd85d7e1f646a09ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#26:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8879dcd0b1f1427ebae01a8d7f4be952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#29:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881d621cc1954682ae14407e1d1a6698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#23:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcf1007d9e7427ab63a7eaf574c9826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#31:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59434366a716447dba9eaee9ef8a7d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#30:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3255 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1400 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5090 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1309 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (898 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1349 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errorsToken indices sequence length is longer than the specified maximum sequence length for this model (1288 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3449 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11510 > 512). Running this sequence through the model will result in indexing errorsToken indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1695 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9311 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f69c33a75444429b3bfa69f5fc16d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77d217bed1c4bb9991544baf037e249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105c97e464bc4ee6ae3c61d7c3767c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e9936c33a74c8d93dd27bdb7672b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a4e14d91d44ff9a68a056f976619d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5daa247c6646f8bb27c79c1ca1117d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ccccf45e064d5aad94ba837f55b6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f01e86834ea4706bd4237ff995d2cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc45c3ff5e33455d87c04e8c29204212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21352441a105491f9d97ed5e23ebc89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69850d46657d4daa9d4420459360be5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78ac523b41a4c5db283576fa999fc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c67d81a56d465ca0b109a37fb08541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4b735888154f4dbb7acb4067c8e8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0700dafe90412189c6fafaa1eb9a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b5c44ea5a24ccaa97b38371666498e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#16:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfc1d5253184656a7add733d5d839f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#17:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f0a11f315c4d81a7b7b67f653cc77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1550 > 512). Running this sequence through the model will result in indexing errorsToken indices sequence length is longer than the specified maximum sequence length for this model (1198 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 512). Running this sequence through the model will result in indexing errors"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1226 > 512). Running this sequence through the model will result in indexing errors"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061ca34e539344ce838753d80b975a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#18:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5a811095e64fa19112c413bdbe71d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#19:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710d4442a72044aaa901e89ab9d8970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#20:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d064c5520bc459488119399f6e72800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#24:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a7bf7b082841f39523c42d7bb49261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#25:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405e7a1220b340d9bcc1462ed2b29712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#26:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aef3e9a52d141e2bf205de054e8b355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#21:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1499 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errorsToken indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1006 > 512). Running this sequence through the model will result in indexing errorsToken indices sequence length is longer than the specified maximum sequence length for this model (1013 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a0eba160b64f5bbf2951ef2506a23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#23:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ee80a1c9c44375b6320e0dab82ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#27:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7714c5ca71d04ec387c1616680febf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#22:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930b06d6e25f43ebb591c02cd59ef7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#28:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc72dab89b4415e9ed20944a2682b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#30:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170405bed81d4eda9750b83c9b7a179b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#31:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242f8446a5554fbd933457a1c892a122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#29:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1438 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9300 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:28:45,108 - __main__ - INFO - Tokenized datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 9000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 1001\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize(article):\n",
    "    tokenized_article = tokenizer(article['text'])\n",
    "    if tokenizer.is_fast:\n",
    "        tokenized_article['word_ids'] = [tokenized_article.word_ids(i) for i in range(len(tokenized_article['input_ids']))]\n",
    "    return tokenized_article\n",
    "\n",
    "\n",
    "logger.info('Tokenizing dataset splits')\n",
    "num_proc = int(os.cpu_count()/N_GPUS)\n",
    "logger.info(f'Total number of processes = {num_proc}')\n",
    "tokenized_datasets = data_splits.map(tokenize, batched=True, num_proc=num_proc, remove_columns=['text'])\n",
    "logger.info(f'Tokenized datasets: {tokenized_datasets}')\n",
    "\n",
    "\n",
    "# Concat and chunk dataset \n",
    "def concat_and_chunk(articles):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {key: sum(articles[key], []) for key in articles.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(articles.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length//CHUNK_SIZE) * CHUNK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    chunked_articles = {key: [text[i : i+CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)] for key, text in concatenated_examples.items()}\n",
    "    # Create a new labels column\n",
    "    chunked_articles['labels'] = chunked_articles['input_ids'].copy()\n",
    "    return chunked_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c97583b7-9c27-49ea-a086-d3a2e00e1f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:33:35,430 - __main__ - INFO - Concatenating and chunking the datasets to a fixed length\n",
      "                                     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3826906ae4c3ba401588b7f2b6f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b1650d179a48028c0be546e8ababa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c128eb6c8c4baab5e78a66b110e52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f80e680a4e743658dab1cdbe128f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bf0992c22f46ff8be3f68ddcd5f1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8b4778887648b79b2c4e8e6a23b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16409d4b36d74416b39c9330faf56c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0c6be6bd86485bab327bd34493729a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9d7e72159f4ea2b773f4b5113eb475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694eab4b28e44927829b4196e841489b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dc4c94ca09401caf381397412c103a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5793efdee16413ca8ed6199a36c5fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679655b565324cb8877ab60a01b6b70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1953381d04724210adb70191fc92c5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d02dbf810454a9f1cb8daa71a088c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7498206dfdc04edea2ee5526b22bbde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c14d4f7e4c488c8855bc0e9a9be598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#16:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4eea515f2f4ef7942de21a7f399285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#17:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd89b744f514b9eaf53d57f43cff584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#19:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d8491f63474fea840f02bd9b5fb3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#18:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e11e1456a34d4aaba7476e9c192baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#20:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea4646762524fbaab7d3197212e5dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#21:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312ff0d9ba9349cda289039601965a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#24:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797049979eab45789db8a3ae6063f556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#26:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e128948876184468aa05742c667c0776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#25:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36df17b8ceb04c78be8909e112c06023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#22:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e26e2e14d26480b8d308a8e02936e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#23:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a14e7d10b84edfbf7d4068584ee85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#28:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05f531638c84c21af55fcbc90a5ae6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#29:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3b5db8e81b471c89b04d4db4502097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#30:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d8db5c59e4f31a0a8779f675dbbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#27:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b482dddd59481c8995d208d65e14cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#31:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318a78a3f3af45d8b6ec6da82fca9ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a9ef7623c9420e813f74dffb55115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424b8a9686064cae86e771c872cf722c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c59f1d2a24c4bc79160e6900be5e85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9febebc43f5e443d8a88c8e3562aff8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2506a7c3c74e8e8619da5f37871219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68295013279f4c4594698b91a3366ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4deb3e99e24454be7f09f681279e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85a324ddde346ab8230f444abadd306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b21e03395a457b904f340807e669d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf0a0479f5d49b0a0b4864f59268f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#10:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28cb797d376490f9eef9ff4a9fe7b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#11:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28d17c38d714ceb8c773e1bf4016ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#12:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2041261503064f86b8a9e68b56ca827b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#13:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a10eb99eb4d4229b12b26470f2ded59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#14:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac213f076084ae188fc248469501dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#15:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89746fc1df1b4061840a28c6efb88cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#17:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1eb0dd65a44e05a6b36d0777ec0c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#16:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3df077f6bc4ff6bcdcc2368a762a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#19:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f60f6f0e6e44884881affe95a01104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#20:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afe55a0bb2942b98a7192b9beb86c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#18:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4713d9d15f64ca19d7ea9b7938cdc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#24:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ed1dd23b8041d882d4cc5ec81fe2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#21:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea823cce1b5c4405a36c2c3104b0888c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#22:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e393de048af5442f851a016938ff9608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#25:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9247f7ca7bb4508b385e464775b3a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#26:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f1181a09764850b50b2e097afdd63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#23:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586b3325c71f4ac5bc3fabe1bd059960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#27:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df73fe4db7ae42d893edf7229b111c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#28:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5730066fa643c5b030e01ff45c29a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#29:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4f98393cfe4324a2dbb293cd84098c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#30:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b12eb1c2c49c48d345ca4644f6ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#31:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:33:42,067 - __main__ - INFO - Chunked datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 76994\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 8127\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "logger.info('Concatenating and chunking the datasets to a fixed length')\n",
    "chunked_datasets = tokenized_datasets.map(concat_and_chunk, batched=True, num_proc=num_proc)\n",
    "logger.info(f'Chunked datasets: {chunked_datasets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6654a99a-a14e-49ba-8837-68f7458c634b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 76994\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 8127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a28b1-6e30-4331-a4f3-932e9fd28b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f912830a-a034-4405-bf17-40612ca18873",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-21 19:43:02,373 - __main__ - INFO - Saving chunked datasets to local disk /tokenized\n",
      "2023-01-21 19:43:03,848 - __main__ - INFO - Validating if datasets were saved correctly\n",
      "2023-01-21 19:43:05,537 - __main__ - INFO - Reloaded dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 76994\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 8127\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Save chunked datasets to local disk (EBS volume)\n",
    "logger.info(f'Saving chunked datasets to local disk {LOCAL_OUTPUT_PATH}')\n",
    "chunked_datasets.save_to_disk('./tokenized')\n",
    "\n",
    "# Validate if datasets were saved correctly\n",
    "logger.info('Validating if datasets were saved correctly')\n",
    "reloaded_dataset = datasets.load_from_disk('./tokenized')\n",
    "logger.info(f'Reloaded dataset: {reloaded_dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ae9af-a8db-47c8-ad84-38be6253f90b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c36b3e30-c934-4253-a8c4-bba8736a159e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b06535d-ce27-4cff-985a-9ea31ee532f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425a8afda49e4a4ca7489a4fb7d64980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4425f742521f4b3d92a981c9cfcd26db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 72555\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 7630\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = data_splits.map(\n",
    "    tokenize, batched=True, remove_columns=data_splits[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d4d43f-9311-4f2a-91a7-2ce055d08b67",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c8ca689-e6b2-46cc-ac3f-02556b6704cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e59e364b-a966-4101-a60a-d0eb39a7aa87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 128,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cd5ea34-a371-4bef-8bb7-050e6a3a6b52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b681f1f3-4cf4-4e57-bf57-36cadc88b1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82734029-0f3b-4cc5-8b01-a9054311b17b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e0e888c-1da5-4464-a761-1b4fe617339e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "867183d2-d90e-49be-b23a-f3e8c83c6f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 72555\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-01-21 19:55:38.393 pytorch-1-8-gpu-py36-ml-g5-8xlarge-59efe6ee5124ceafd0c624e661a4:1013 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-01-21 19:55:38.422 pytorch-1-8-gpu-py36-ml-g5-8xlarge-59efe6ee5124ceafd0c624e661a4:1013 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='283' max='283' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [283/283 07:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=283, training_loss=7.269319891508392, metrics={'train_runtime': 449.7699, 'train_samples_per_second': 161.316, 'train_steps_per_second': 0.629, 'total_flos': 4732521283584000.0, 'train_loss': 7.269319891508392, 'epoch': 1.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027bae2-8100-4790-9e3e-643155a1d956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54f9e5b2-1233-4a63-8f57-7d1bea21d23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model\n",
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/tokenizer_config.json\n",
      "Special tokens file saved in ./model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0773e-085b-41ef-9cf6-6b8aaff235f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca01645d-b3a8-443c-82cd-e361a93b3756",
   "metadata": {},
   "source": [
    "#### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "482ded76-b77a-4ba6-88e1-d1167da60acb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d5174d5-5fb8-4101-920a-3debd4f2bc85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49ad710d-0203-4652-bf0f-bb5a4f928136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fb6dbdb26a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1caac58-a281-465d-b148-6ec502e55072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Tokyo is the capital of Japan, making them the second most densely populated nation in India, a top four destination for mobile phone users.\\n\\nAs to the problem of the phone's not working for them, according to some analysts, the problem\"}]\n"
     ]
    }
   ],
   "source": [
    "txt = 'Tokyo is the capital of'\n",
    "print(pipe(txt, num_return_sequences=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a709469-ec71-42b2-9b7a-d17fb0e2c88c",
   "metadata": {},
   "source": [
    "### Creating a custom pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8193afc2-87da-4f7d-b981-19dbb183c62e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./model/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./model\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file ./model/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./model\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 128,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Didn't find file ./model/added_tokens.json. We won't load it.\n",
      "loading file ./model/vocab.json\n",
      "loading file ./model/merges.txt\n",
      "loading file ./model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./model/special_tokens_map.json\n",
      "loading file ./model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d3cb0-3487-4b16-966e-17e1000f8b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "673133e0-a57f-4bcd-97df-74a8c1071e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Covid medicine, which has been more than 7.5.4% in the covid-19 cases and that the most of $ 300 million.4 million. the covid-19 has, and the federal ministry of the world to our global health'}]\n"
     ]
    }
   ],
   "source": [
    "txt = 'Covid medicine'\n",
    "print(pipe(txt, num_return_sequences=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d01dd-65b9-4abd-90c9-f5819077bf48",
   "metadata": {},
   "source": [
    "##### How to fine-tune --- for downstream tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506620c-ae7d-443f-b5e0-8218023c9328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecdac498-3118-4ab4-892e-d6d57c4c2bbc",
   "metadata": {},
   "source": [
    "#### Direct inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6befa2aa-d5d2-4ec5-8220-aab08cec786b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Below are some examples for sentiment detection of movie reviews.\n",
      "\n",
      "Review: I am sad that the hero died.\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was perfect.\n",
      "Sentiment: Positive\n",
      "\n",
      "Review: The plot was not so good!\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n",
      "\n",
      "Sentiment: Negative\n",
      "\n",
      "Review: The ending was not so good!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Output --> \\nBelow are some examples for sentiment detection of movie reviews.\\n\\nReview: I am sad that the hero died.\\nSentiment: Negative\\n\\nReview: The ending was perfect.\\nSentiment: Positive\\n\\nReview: The plot was not so good!\\nSentiment: Negative\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "# create prompt\n",
    "prompt = \"\"\"\n",
    "Below are some examples for sentiment detection of movie reviews.\n",
    "\n",
    "Review: I am sad that the hero died.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: The ending was perfect.\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: The plot was not so good!\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# generate tokens\n",
    "generated = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "# perform prediction \n",
    "sample_outputs = model.generate(generated, do_sample=False, top_k=50, max_length=512, top_p=0.90, \n",
    "        temperature=0, num_return_sequences=0)\n",
    "\n",
    "# decode the predicted tokens into texts\n",
    "predicted_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "print(predicted_text)\n",
    "\"\"\"Output --> \n",
    "Below are some examples for sentiment detection of movie reviews.\n",
    "\n",
    "Review: I am sad that the hero died.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: The ending was perfect.\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: The plot was not so good!\n",
    "Sentiment: Negative\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f536a2e-03c2-46cb-a210-8c43026e017d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.8-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
